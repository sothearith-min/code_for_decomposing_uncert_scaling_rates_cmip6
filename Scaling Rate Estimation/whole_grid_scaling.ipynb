{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data Preparation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "def calculate_dewpoint(temp, humidity):\n",
    "    A = 17.27\n",
    "    B = 237.7\n",
    "    alpha = ((A * temp) / (B + temp)) + np.log(humidity/100.0)\n",
    "    return (B * alpha) / (A - alpha)\n",
    "\n",
    "def reading_data(pr_dir, tas_dir, hur_dir, start_year, stop_year):\n",
    "    years_list = [str(year) for year in range(start_year, stop_year)]\n",
    "    \n",
    "    interp_pr, interp_tas, interp_hur, interp_tdew = {}, {}, {}, {}\n",
    "\n",
    "    for year in years_list: \n",
    "        file_name = f\"{year}.csv\"\n",
    "        # pr\n",
    "        file_pr = pd.read_csv(pr_dir + \"\\\\\" + file_name)\n",
    "        file_pr = file_pr[file_pr.lat > -60]\n",
    "        # tas\n",
    "        file_tas = pd.read_csv(tas_dir+ \"\\\\\" + file_name)\n",
    "        file_tas = file_tas[file_tas.lat > -60]\n",
    "        # hur\n",
    "        file_hur = pd.read_csv(hur_dir+ \"\\\\\" + file_name)\n",
    "        file_hur = file_hur[file_hur.lat > -60]\n",
    "        interp_pr[year], interp_tas[year], interp_hur[year] = file_pr, file_tas, file_hur \n",
    "    \n",
    "    # Calculate Dewpoint temperature \n",
    "\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    interp_tdew = {}\n",
    "\n",
    "    for key in interp_tas.keys():\n",
    "        tas = interp_tas[key]\n",
    "        hur = interp_hur[key]\n",
    "        tdew = pd.DataFrame()\n",
    "        tdew[[\"lat\", \"lon\"]] = tas[[\"lat\", \"lon\"]]\n",
    "        for day in tas.columns[2:]:\n",
    "            tdew[day] = calculate_dewpoint(tas[day], hur[day])\n",
    "        interp_tdew[key] = tdew \n",
    "    \n",
    "    # Combine pr and tdew \n",
    "    pr = pd.concat([df.set_index([\"lat\", \"lon\"]) for df in list(interp_pr.values())], axis=1).reset_index()\n",
    "    pr['lat_lon'] = pr['lat'].astype(str) + ',' + pr['lon'].astype(str)\n",
    "    pr = pr.drop(['lat', 'lon'], axis=1)\n",
    "    pr_long = pd.melt(pr, id_vars=['lat_lon'], var_name='date', value_name='pr')\n",
    "    tdew = pd.concat([df.set_index([\"lat\", \"lon\"]) for df in list(interp_tas.values())], axis=1).reset_index()\n",
    "    tdew['lat_lon'] = tdew['lat'].astype(str) + ',' +  tdew['lon'].astype(str)\n",
    "    tdew = tdew.drop(['lat', 'lon'], axis=1)\n",
    "    tdew_long = pd.melt(tdew, id_vars=['lat_lon'], var_name='date', value_name='tdew')\n",
    "    tdew_values = tdew_long['tdew'].values\n",
    "    truncated_values = tdew_values[:len(pr_long.index)]\n",
    "    # Assign the truncated values to the DataFrame column\n",
    "    pr_long[\"tdew\"] = truncated_values\n",
    "   \n",
    "    pr_long = pr_long[pr_long.pr >= 0.1]\n",
    "    \n",
    "    return pr_long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Quantiel Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fit_quantile_regression(group, q):\n",
    "    model = sm.QuantReg(np.log(group['pr']), sm.add_constant(group['tdew']))\n",
    "    result = model.fit(q=q)\n",
    "    return result.params['tdew']\n",
    "\n",
    "def quantile_regression(df, q):\n",
    "    grouped = df.groupby('lat_lon')\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        slopes = list(executor.map(lambda group: fit_quantile_regression(group, q), [group for name, group in grouped]))\n",
    "\n",
    "    slope_df = pd.DataFrame({\n",
    "        'lat_lon': grouped.groups.keys(),\n",
    "        'slope': slopes\n",
    "    })\n",
    "    \n",
    "    slope_df['scaling'] = 100 * (np.exp(slope_df['slope']) - 1)\n",
    "\n",
    "    print(\"done --> qr\")\n",
    "    \n",
    "    return slope_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Binning with equal bin width***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "def binning_w(df, q):\n",
    "\n",
    "    tdew_min = df['tdew'].min()\n",
    "    tdew_max = df['tdew'].max()\n",
    "    bin_width = 2\n",
    "    tdew_bins = np.arange(tdew_min, tdew_max + bin_width, bin_width)\n",
    "    \n",
    "    df['tdew_bin'] = df.groupby('lat_lon')['tdew'].transform(lambda x: np.digitize(x, bins=tdew_bins))\n",
    "    \n",
    "    df_avg_tdew = df.groupby(['lat_lon', 'tdew_bin'])['tdew'].mean().reset_index()\n",
    "    df_avg_tdew.rename(columns={'tdew': 'avg_tdew'}, inplace=True)\n",
    "    \n",
    "    df_percentiles_pr = df.groupby(['lat_lon', 'tdew_bin'])['pr'].quantile(q).reset_index()\n",
    "    df_percentiles_pr.rename(columns={'pr': '99th_percentile_pr'}, inplace=True)\n",
    "\n",
    "    df_final = df_avg_tdew.merge(df_percentiles_pr, on=['lat_lon', 'tdew_bin'], how='left')\n",
    "\n",
    "    coefficients = defaultdict(float)\n",
    "    for lat_lon, group in df_final.groupby('lat_lon'):\n",
    "        X = group['avg_tdew'].values.reshape(-1, 1)\n",
    "        y = group['99th_percentile_pr'].values\n",
    "    \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, np.log(y))\n",
    "    \n",
    "        slope = model.coef_[0]  \n",
    "        coefficients[lat_lon] = slope\n",
    "\n",
    "    result_df = pd.DataFrame(list(coefficients.items()), columns=['lat_lon', 'slope'])\n",
    "    result_df[\"scaling\"] = 100 * (np.exp(result_df['slope']) - 1)\n",
    "    result_df[['lat', 'lon']] = result_df['lat_lon'].str.split(',', expand=True)\n",
    "    result_df['lat'] = result_df['lat'].astype(float)\n",
    "    result_df['lon'] = result_df['lon'].astype(float)\n",
    "    result_df.drop(columns=['lat_lon'], inplace=True)\n",
    "\n",
    "    print(\"done --> binning_w\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Binning with Equal Sample Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def binning_p(df, q, loess_frac=0.1):\n",
    "    def _calculate_stats(group):\n",
    "        group['pr_bin'] = pd.qcut(group['tdew'], q=30, labels=False, duplicates='drop')\n",
    "        group['p99_pr'] = group.groupby('pr_bin')['pr'].transform(lambda x: x.quantile(q))\n",
    "        group['avg_tdew'] = group.groupby('pr_bin')['tdew'].transform('mean')\n",
    "        return group[['lat_lon', 'pr_bin', 'p99_pr', 'avg_tdew']].drop_duplicates()\n",
    "\n",
    "    def has_peak_point(x, y):\n",
    "        diff_sign = np.sign(np.diff(y))\n",
    "        if np.all(diff_sign[:-1] == diff_sign[1:]):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def fit_loess(df, frac):\n",
    "        smoothed_dfs = []\n",
    "\n",
    "        for lat_lon, group in df.groupby('lat_lon'):\n",
    "            smoothed = lowess(group['p99_pr'], group['avg_tdew'], frac=frac, return_sorted=False)\n",
    "            smoothed_df = pd.DataFrame({\n",
    "                'lat_lon': lat_lon,\n",
    "                'avg_tdew': group['avg_tdew'],\n",
    "                'p99_pr_smooth': smoothed\n",
    "            })\n",
    "            smoothed_dfs.append(smoothed_df)\n",
    "\n",
    "        smoothed_df = pd.concat(smoothed_dfs, ignore_index=True)\n",
    "\n",
    "        return smoothed_df\n",
    "    \n",
    "    def detect_peak_points(df):\n",
    "        peak_points = []\n",
    "\n",
    "        for lat_lon, group in df.groupby('lat_lon'):\n",
    "            if has_peak_point(group['avg_tdew'], group['p99_pr_smooth']):\n",
    "                peak_point_idx = np.argmax(group['p99_pr_smooth'])\n",
    "                peak_point = group.iloc[peak_point_idx]\n",
    "                peak_points.append(peak_point)\n",
    "\n",
    "        peak_points_df = pd.DataFrame(peak_points)\n",
    "\n",
    "        return peak_points_df\n",
    "\n",
    "    def fit_linear_regression(df, peak_points_df):\n",
    "        regression_dfs = []\n",
    "\n",
    "        for lat_lon, group in df.groupby('lat_lon'):\n",
    "            if lat_lon in peak_points_df['lat_lon'].values:\n",
    "                peak_point_idx = peak_points_df[peak_points_df['lat_lon'] == lat_lon].index[0]\n",
    "                group_before_peak = group.iloc[:peak_point_idx]\n",
    "                if peak_point_idx == 0:\n",
    "                    group_before_peak = group\n",
    "                else:\n",
    "                    group_before_peak = group.iloc[:peak_point_idx]\n",
    "            else:\n",
    "                group_before_peak = group\n",
    "\n",
    "            X = group_before_peak[['avg_tdew']]\n",
    "            y = np.log(group_before_peak['p99_pr_smooth'].abs())\n",
    "\n",
    "            if y.isnull().any():\n",
    "                group_before_peak = group_before_peak.dropna(subset=['p99_pr_smooth'])\n",
    "                X = group_before_peak[['avg_tdew']]\n",
    "                y = np.log(group_before_peak['p99_pr_smooth'].abs())\n",
    "\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            slope = model.coef_[0]\n",
    "            intercept = model.intercept_\n",
    "\n",
    "            regression_df = pd.DataFrame({\n",
    "                'lat_lon': lat_lon,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept\n",
    "            }, index=[0])\n",
    "\n",
    "            regression_dfs.append(regression_df)\n",
    "\n",
    "        regression_df = pd.concat(regression_dfs, ignore_index=True)\n",
    "        regression_df[\"scaling\"] = 100*(np.exp(regression_df[\"slope\"])-1)\n",
    "        return regression_df\n",
    "    \n",
    "  \n",
    "    result_df = df.groupby('lat_lon').apply(_calculate_stats).reset_index(drop=True)\n",
    "    \n",
    "    smoothed_df = fit_loess(result_df, frac=loess_frac)\n",
    "    \n",
    "    peak_points_df = detect_peak_points(smoothed_df)\n",
    "    \n",
    "    regression_df = fit_linear_regression(smoothed_df, peak_points_df)\n",
    "    \n",
    "    print(\"done --> binning_p\")\n",
    "    return regression_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Apply Functions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Noted: file_name here refers to location a csv file which is a location of files in each year for each variables, in each models/scenarios***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = pd.read_csv(r\"file_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- hadgem2-es x  rcp85 ---------------\n",
      "done --> qr\n",
      "done --> qr\n",
      "done --> binning_w\n",
      "done --> binning_w\n",
      "done --> binning_p\n",
      "done --> binning_p\n"
     ]
    }
   ],
   "source": [
    "for file in range(len(file_dir)):\n",
    "    model = file_dir[\"Model\"][file]\n",
    "    scenario = file_dir[\"Scenario\"][file]\n",
    "    pr_dir = file_dir[\"Pr\"][file]\n",
    "    tas_dir = file_dir[\"Tas\"][file]\n",
    "    hurs_dir = file_dir[\"Hurs\"][file]\n",
    "\n",
    "    data = reading_data(pr_dir, tas_dir, hurs_dir, start_year = 2041, stop_year = 2091)\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "    data = data.dropna(subset=['date'])\n",
    "    data[\"year\"] = data['date'].dt.year\n",
    "    data.drop('date', axis = 1, inplace = True)\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    print('---------------', model , 'x ', scenario, '---------------')\n",
    "    qr99 = quantile_regression(df, 0.99)\n",
    "    qr99.to_csv(r\"folder_to_save\\\\q99_qr_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "         \n",
    "    qr95 = quantile_regression(df, 0.95)\n",
    "    qr95.to_csv(r\"folder_to_save\\\\q95_qr_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "         \n",
    "    bw99 = binning_w(df, 0.99)\n",
    "    bw99.to_csv(r\"folder_to_save\\\\q99_binning-w_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "         \n",
    "    bw95 = binning_w(df, 0.95)\n",
    "    bw95.to_csv(r\"folder_to_save\\\\q95_binning-w_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "         \n",
    "    bp99 = binning_p(df, 0.99)\n",
    "    bp99.to_csv(r\"folder_to_save\\\\q99_binning-p_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "         \n",
    "    bp95 = binning_p(df, 0.95)\n",
    "    bp95.to_csv(r\"folder_to_save\\\\q95_binning-p_whole_grid_\" + model + \"-\" + scenario + \".csv\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
