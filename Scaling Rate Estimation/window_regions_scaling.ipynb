{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Reading all CVS files for each year: all 3 variable***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER2\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import warnings \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Dew Point Temperature Calculation\n",
    "\n",
    "def calculate_dewpoint(temp, humidity):\n",
    "    A = 17.27\n",
    "    B = 237.7\n",
    "    alpha = ((A * temp) / (B + temp)) + np.log(humidity/100.0)\n",
    "    return (B * alpha) / (A - alpha)\n",
    "\n",
    "def reading_data(pr_dir, tas_dir, hur_dir, start_year, stop_year):\n",
    "    years_list = [str(year) for year in range(start_year, stop_year)]\n",
    "    \n",
    "    interp_pr, interp_tas, interp_hur, interp_tdew = {}, {}, {}, {}\n",
    "\n",
    "    for year in years_list: \n",
    "        file_name = f\"{year}.csv\"\n",
    "\n",
    "        file_pr = pd.read_csv(pr_dir + \"\\\\\" + file_name)\n",
    "        file_pr = file_pr[file_pr.lat > -60]\n",
    "\n",
    "        file_tas = pd.read_csv(tas_dir+ \"\\\\\" + file_name)\n",
    "        file_tas = file_tas[file_tas.lat > -60]\n",
    "\n",
    "        file_hur = pd.read_csv(hur_dir+ \"\\\\\" + file_name)\n",
    "        file_hur = file_hur[file_hur.lat > -60]\n",
    "        interp_pr[year], interp_tas[year], interp_hur[year] = file_pr, file_tas, file_hur \n",
    "    \n",
    "\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    interp_tdew = {}\n",
    "\n",
    "    for key in interp_tas.keys():\n",
    "        tas = interp_tas[key]\n",
    "        hur = interp_hur[key]\n",
    "        tdew = pd.DataFrame()\n",
    "        tdew[[\"lat\", \"lon\"]] = tas[[\"lat\", \"lon\"]]\n",
    "        for day in tas.columns[2:]:\n",
    "            tdew[day] = calculate_dewpoint(tas[day], hur[day])\n",
    "        interp_tdew[key] = tdew \n",
    "    \n",
    "    pr = pd.concat([df.set_index([\"lat\", \"lon\"]) for df in list(interp_pr.values())], axis=1).reset_index()\n",
    "    pr['lat_lon'] = pr['lat'].astype(str) + ',' + pr['lon'].astype(str)\n",
    "    pr = pr.drop(['lat', 'lon'], axis=1)\n",
    "    pr_long = pd.melt(pr, id_vars=['lat_lon'], var_name='date', value_name='pr')\n",
    "    tdew = pd.concat([df.set_index([\"lat\", \"lon\"]) for df in list(interp_tas.values())], axis=1).reset_index()\n",
    "    tdew['lat_lon'] = tdew['lat'].astype(str) + ',' +  tdew['lon'].astype(str)\n",
    "    tdew = tdew.drop(['lat', 'lon'], axis=1)\n",
    "    tdew_long = pd.melt(tdew, id_vars=['lat_lon'], var_name='date', value_name='tdew')\n",
    "    pr_long[\"tdew\"] = tdew_long['tdew'].values\n",
    "    pr_long = pr_long[pr_long.pr >= 0.1]\n",
    "    grouped = pd.read_csv(r\"https://raw.githubusercontent.com/sothearith-min/code_for_decomposing_uncert_scaling_rates_cmip6/main/Scaling%20Rate%20Estimation/Grouping_Region_SREX.csv?token=GHSAT0AAAAAACW4IMIQAJ7JBBJHD3BYZH3WZWVHOSA\", index_col=0)[[\"lat\", \"lon\", \"code\"]] # Merging region_name With region_location\n",
    "    grouped['lat_lon'] = grouped['lat'].astype(str) + ',' + grouped['lon'].astype(str)\n",
    "    grouped = grouped.drop(['lat', 'lon'], axis=1)\n",
    "    df = pd.merge(pr_long, grouped, on = 'lat_lon', how = 'inner')\n",
    "    df = df.drop('lat_lon', axis = 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Binning with equal bin width***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import defaultdict\n",
    "\n",
    "def binning_w(data, q):\n",
    "\n",
    "    tdew_min = data['tdew'].min()\n",
    "    tdew_max = data['tdew'].max()\n",
    "    bin_width = 2\n",
    "    tdew_bins = np.arange(tdew_min, tdew_max + bin_width, bin_width)\n",
    "    \n",
    "    data['tdew_bin'] = data.groupby('code')['tdew'].transform(lambda x: np.digitize(x, bins=tdew_bins))\n",
    "    \n",
    "    df_avg_tdew = data.groupby(['code', 'tdew_bin'])['tdew'].mean().reset_index()\n",
    "    df_avg_tdew.rename(columns={'tdew': 'avg_tdew'}, inplace=True)\n",
    "    \n",
    "    df_percentiles_pr = data.groupby(['code', 'tdew_bin'])['pr'].quantile(q).reset_index()\n",
    "    df_percentiles_pr.rename(columns={'pr': '99th_percentile_pr'}, inplace=True)\n",
    "\n",
    "    df_final = df_avg_tdew.merge(df_percentiles_pr, on=['code', 'tdew_bin'], how='left')\n",
    "\n",
    "    coefficients = defaultdict(float)\n",
    "    for code, group in df_final.groupby('code'):\n",
    "        X = group['avg_tdew'].values.reshape(-1, 1)\n",
    "        y = group['99th_percentile_pr'].values\n",
    "    \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, np.log(y))\n",
    "    \n",
    "        slope = model.coef_[0]  \n",
    "        coefficients[code] = slope\n",
    "\n",
    "    result_df = pd.DataFrame(list(coefficients.items()), columns=['code', 'slope'])\n",
    "    result_df[\"scaling\"] = 100 * (np.exp(result_df['slope']) - 1)\n",
    "\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Binning with equal sample points***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def binning_p(df, q, loess_frac=0.1):\n",
    "    def _calculate_stats(group):\n",
    "        group['pr_bin'] = pd.qcut(group['tdew'], q=30, labels=False, duplicates='drop')\n",
    "        group['p99_pr'] = group.groupby('pr_bin')['pr'].transform(lambda x: x.quantile(q))\n",
    "        group['avg_tdew'] = group.groupby('pr_bin')['tdew'].transform('mean')\n",
    "        return group[['code', 'pr_bin', 'p99_pr', 'avg_tdew']].drop_duplicates()\n",
    "\n",
    "    def has_peak_point(x, y):\n",
    "        diff_sign = np.sign(np.diff(y))\n",
    "        if np.all(diff_sign[:-1] == diff_sign[1:]):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def fit_loess(df, frac):\n",
    "        smoothed_dfs = []\n",
    "\n",
    "        for code, group in df.groupby('code'):\n",
    "            smoothed = lowess(group['p99_pr'], group['avg_tdew'], frac=frac, return_sorted=False)\n",
    "            smoothed_df = pd.DataFrame({\n",
    "                'code': code,\n",
    "                'avg_tdew': group['avg_tdew'],\n",
    "                'p99_pr_smooth': smoothed\n",
    "            })\n",
    "            smoothed_dfs.append(smoothed_df)\n",
    "\n",
    "        smoothed_df = pd.concat(smoothed_dfs, ignore_index=True)\n",
    "\n",
    "        return smoothed_df\n",
    "    \n",
    "    def detect_peak_points(df):\n",
    "        peak_points = []\n",
    "\n",
    "        for code, group in df.groupby('code'):\n",
    "            if has_peak_point(group['avg_tdew'], group['p99_pr_smooth']):\n",
    "                peak_point_idx = np.argmax(group['p99_pr_smooth'])\n",
    "                peak_point = group.iloc[peak_point_idx]\n",
    "                peak_points.append(peak_point)\n",
    "\n",
    "        peak_points_df = pd.DataFrame(peak_points)\n",
    "\n",
    "        return peak_points_df\n",
    "\n",
    "    def fit_linear_regression(df, peak_points_df):\n",
    "        regression_dfs = []\n",
    "\n",
    "        for code, group in df.groupby('code'):\n",
    "            if code in peak_points_df['code'].values:\n",
    "                peak_point_idx = peak_points_df[peak_points_df['code'] == code].index[0]\n",
    "                group_before_peak = group.iloc[:peak_point_idx]\n",
    "                if peak_point_idx == 0:\n",
    "                    group_before_peak = group\n",
    "                else:\n",
    "                    group_before_peak = group.iloc[:peak_point_idx]\n",
    "            else:\n",
    "                group_before_peak = group\n",
    "\n",
    "            X = group_before_peak[['avg_tdew']]\n",
    "            y = np.log(group_before_peak['p99_pr_smooth'].abs())\n",
    "\n",
    "            if y.isnull().any():\n",
    "                group_before_peak = group_before_peak.dropna(subset=['p99_pr_smooth'])\n",
    "                X = group_before_peak[['avg_tdew']]\n",
    "                y = np.log(group_before_peak['p99_pr_smooth'].abs())\n",
    "\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            slope = model.coef_[0]\n",
    "            intercept = model.intercept_\n",
    "\n",
    "            regression_df = pd.DataFrame({\n",
    "                'code': code,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept\n",
    "            }, index=[0])\n",
    "\n",
    "            regression_dfs.append(regression_df)\n",
    "\n",
    "        regression_df = pd.concat(regression_dfs, ignore_index=True)\n",
    "        regression_df[\"scaling\"] = 100*(np.exp(regression_df[\"slope\"])-1)\n",
    "        return regression_df\n",
    "    \n",
    "  \n",
    "    result_df = df.groupby('code').apply(_calculate_stats).reset_index(drop=True)\n",
    "    \n",
    "    smoothed_df = fit_loess(result_df, frac=loess_frac)\n",
    "    \n",
    "    peak_points_df = detect_peak_points(smoothed_df)\n",
    "    \n",
    "    regression_df = fit_linear_regression(smoothed_df, peak_points_df)\n",
    "    \n",
    "    return regression_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Quantile regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fit_quantile_regression(group, q):\n",
    "    model = sm.QuantReg(np.log(group['pr']), sm.add_constant(group['tdew']))\n",
    "    result = model.fit(q=q)\n",
    "    return result.params['tdew']\n",
    "\n",
    "def quantile_regression(df, q):\n",
    "    grouped = df.groupby('code')\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        slopes = list(executor.map(lambda group: fit_quantile_regression(group, q), [group for name, group in grouped]))\n",
    "\n",
    "    slope_df = pd.DataFrame({\n",
    "        'code': grouped.groups.keys(),\n",
    "        'slope': slopes\n",
    "    })\n",
    "    \n",
    "    slope_df['scaling'] = 100 * (np.exp(slope_df['slope']) - 1)\n",
    "\n",
    "    return slope_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Apply Functions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Noted: file_name here refers to location a csv file which is a location of files in each year for each variables, in each models/scenarios***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = pd.read_csv(r\"file_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in range(len(file_dir)):\n",
    "    model = file_dir[\"Model\"][file]\n",
    "    scenario = file_dir[\"Scenario\"][file]\n",
    "    pr_dir = file_dir[\"Pr\"][file]\n",
    "    tas_dir = file_dir[\"Tas\"][file]\n",
    "    hurs_dir = file_dir[\"Hurs\"][file]\n",
    "\n",
    "    data = reading_data(pr_dir, tas_dir, hurs_dir, start_year = 2041, stop_year = 2091)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data[\"year\"] = data['date'].dt.year\n",
    "    data.drop('date', axis = 1, inplace = True)\n",
    "\n",
    "    print('---------------', model , 'x ', scenario, '---------------')\n",
    "\n",
    "\n",
    "    for y in range(2041, 2072):\n",
    "            \n",
    "            print(\"----------->  \", y  )\n",
    "            \n",
    "            df = data[(data['year'] >= y) & (data['year'] <= y+19)]\n",
    "\n",
    "            #QR99\n",
    "            qr99 = quantile_regression(df, 0.99)\n",
    "            qr99.to_csv(r\"folder_to_save\\\\q99_qr_window_region_\" + model + \"-\" + scenario + \"_\" + str(y)  + \".csv\")\n",
    "            #QR95\n",
    "            qr95 = quantile_regression(df, 0.95)\n",
    "            qr95.to_csv(r\"folder_to_save\\quantile_regression\\\\q99_qr_window_region_\" + model + \"-\" + scenario + \"_\" + str(y) + \".csv\")\n",
    "            #BW99\n",
    "            bw99 = binning_w(df, 0.99)\n",
    "            bw99.to_csv(r\"folder_to_save\\binning_w\\\\q99_binning-w_window_region_\" + model + \"-\" + scenario + \"_\" + str(y)  + \".csv\")\n",
    "            #BW95\n",
    "            bw95 = binning_w(df, 0.95)\n",
    "            bw95.to_csv(r\"folder_to_save\\binning_w\\\\q95_binning-w_window_region_\" + model + \"-\" + scenario + \"_\" + str(y) + \".csv\")\n",
    "            #BP99\n",
    "            bp99 = binning_p(df, 0.99)\n",
    "            bp99.to_csv(r\"folder_to_save\\binning_p\\\\q99_binning-p_window_region_\" + model + \"-\" + scenario + \"_\" + str(y) + \".csv\")\n",
    "            #BP95\n",
    "            bp95 = binning_p(df, 0.95)\n",
    "            bp95.to_csv(r\"folder_to_save\\\\q95_binning-p_window_region_\" + model + \"-\" + scenario + \"_\" + str(y) + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
